{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Gilbert Strang's 2018 Matrix Methods course\n",
    "\n",
    "- Linear algebra => Optimisation => Deep learning\n",
    "- Linear Algebra => Statistics => Deep learning\n",
    "- [\"Learning from data\" book](math.mit.edu/learningfromdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 01: The column space of $A$ contains all vectors $A x$\n",
    "\n",
    "- Think of the product $A x$ as a linear combination of the columns of $A$: $A x = A_{:,1} x_1 + A_{:,2} x_2 + \\ldots + A_{:,n} x_n$\n",
    "- $A = C R$ where the columns of $C$ form a basis for $C(A)$, and each column in $C$ is a column of $A$; then $R$ is the first $rank(A)$ rows of (a column-permutation of) $rref(A)$.\n",
    "- Given $C$, a matrix formed from $r = rank(A)$ l.i. columns of $A$, and $R$, a matrix formed from $r$ l.i. rows of $A$, then there is a matrix $U$ such that $A = C U R$, and $U$ is an $r \\times r$ invertible matrix\n",
    "  - Question: Are there any more properties of $U$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 02: Multiplying and factoring matrices\n",
    "\n",
    "### 5 key factorisations\n",
    "\n",
    "- $A = L U$ -- Elimination\n",
    "- $A = Q R$ -- Gram-Schmidt decomposition\n",
    "- $S = Q \\Lambda Q^T$ -- Spectral theorem (for symmetric matrices $S$)\n",
    "- $A = X \\Lambda X^{-1}$ -- Doesn't work for all matrices\n",
    "- $A = U \\Sigma V^T$ -- Singular Value Decomposition; works for all matrices; orthogonal * diagonal * orthogonal\n",
    "\n",
    "### LU decomposition in rank-1 picture\n",
    "\n",
    "- $A = l_1 u_1^T + \\begin{pmatrix}0 & 0 \\\\ 0 & l_2 u_2^T \\end{pmatrix} + \\begin{pmatrix}0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & l_3 u_3^T \\end{pmatrix} + \\ldots $\n",
    "\n",
    "### Orthogonality of fundamental spaces of a matrix $A$\n",
    "\n",
    "- $C(A^T)$ is orthogonal to $N(A)$ \n",
    "  - i.e. because each row in $A$ is orthogonal to any vector in $N(A)$\n",
    "- $C(A)$ is orthogonal to $N(A^T)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 03: Orthonormal columns in $Q$ give $Q^T Q = I$\n",
    "\n",
    "$Q$ is used to denote a matrix with orthonormal columns - that is, $q_{:,i}^T q_{:,j} = \\delta_{i,j}$.\n",
    "\n",
    "Thus:\n",
    "- $Q^T Q = I_m$, and\n",
    "- $Q Q^T = \\begin{pmatrix}I_n & 0 \\\\ 0 & 0\\end{pmatrix}$\n",
    "\n",
    "If $Q^T Q = Q Q^T = I$, then $Q$ is 'orthogonal'.\n",
    "\n",
    "### Orthogonal matrices preserve length under $l_2$\n",
    "\n",
    "i.e. $|Q x| = |x|$\n",
    "\n",
    "**proof**: $|Q x|^2 = |(Q x)^T (Q x)| = |x^T (Q^T Q) x| = |x^T x| = |x|^2$\n",
    "\n",
    "### Examples of orthogonal matrices\n",
    "\n",
    "#### rotation matrices\n",
    "$\\begin{pmatrix} cos{\\theta} & sin{\\theta} \\\\ -sin{\\theta} & cos{\\theta}\\end{pmatrix}$\n",
    "\n",
    "Rotates anti-clockwise by $\\theta$ around the origin in 2-d\n",
    "  \n",
    "#### reflection matrices\n",
    "$\\begin{pmatrix} cos{\\theta} & sin{\\theta} \\\\ sin{\\theta} & -cos{\\theta}\\end{pmatrix}$\n",
    "\n",
    "Reflects plane in the line at $\\theta/2$\n",
    "\n",
    "#### \"Householder reflections\"\n",
    "Given unit vector $u$, then $H = I - 2 u u^T$\n",
    "\n",
    "#### \"Hadamard\" matrices\n",
    "\n",
    "$H_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$\n",
    "\n",
    "$H_{2^n} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} H_{2^{n-1}} & H_{2^{n-1}} \\\\ H_{2^{n-1}} & -H_{2^{n-1}} \\end{pmatrix}$\n",
    "\n",
    "**Conjecture**: There is an orthogonal matrix of size $n \\times n$ with entries $1$ and $-1$ for $n$ a multiple of $4$ --- known up to $n=668$.\n",
    "\n",
    "#### Wavelets\n",
    "\n",
    "$W_4 = \\begin{pmatrix}\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 &-1 & 0 \\\\\n",
    "1 &-1 & 0 & 1 \\\\\n",
    "1 &-1 & 0 &-1\n",
    "\\end{pmatrix}$\n",
    "\n",
    "(with some scaling on the columns to make them orthonormal)\n",
    "\n",
    "Haar invented in 1910; Ingrid Daubechies 1988 - found families of wavelets with entries that were not just 1 and -1.\n",
    "\n",
    "#### Eigenvectors of a symmetric matrix\n",
    "\n",
    "Example: discrete fourier transform is the matrix of eigenvectors of $Q^T Q$, with $Q = P_{2,3,\\ldots,n-1,n,1}$ (i.e. $Q$ is the permutation matrix that puts row 2 in row 1, row 3 in row 2, etc.)\n",
    "\n",
    "*I didn't understand this bit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 04: Eigenvalues and Eigenvectors\n",
    "\n",
    "Useful because they allow you to work with powers of matrices.\n",
    "\n",
    "The eigenvectors of a general matrix $A$ are not necessarily orthogonal to each other. (*Find an example*)\n",
    "\n",
    "### Similar matrices\n",
    "**Definition**: $B$ is *similar* to $A$ if there exists a matrix $M$ such that $B = M^{-1} A M$.\n",
    "\n",
    "If $B$ is similar to $A$, then they have the same eigenvalues. (Easy to prove)\n",
    "\n",
    "Corollary: (invertible) $A B$ and $B A$ have the same non-zero eigenvalues.  (Just use $M = B$).\n",
    "\n",
    "Computing eigenvalues of $A$ --- usually involves picking better and better values of $M$ to find a triangular matrix similar to $A$. (*How does this work?*)\n",
    "\n",
    "### (Real) Symmetric matrices\n",
    "- Have real eigenvalues (*prove this*)\n",
    "- Have orthogonal eigenvectors (*prove this*)\n",
    "- And thus $Q$, the matrix with eigenvectors as columns is orthonormal, and\n",
    "- $S = Q \\Lambda Q^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 05: Positive definite and semidefinite matrices\n",
    "\n",
    "### Symmetric positive definite matrices\n",
    "Equivalent definitions:\n",
    " 1. All eigenvalues are real and positive ($\\lambda_i > 0$)\n",
    " 2. Energy $x^T S x > 0$ for any $x \\ne 0$\n",
    " 3. $S = A^T A$ (independent columns in $A$)\n",
    " 4. All leading determinants are > 0\n",
    " 5. All pivots in elimination are > 0\n",
    " \n",
    "**Example:**\n",
    "$S = \\begin{pmatrix}3 & 4 \\\\ 4 & 5\\end{pmatrix}$ is *not* positive definite (its determinant is $|A| = 15 - 16 = -1$).\n",
    "\n",
    "$x^T S x$ is a quadratic form -- if $S$ is positive definite, then $S$ is convex with a unique minimum.\n",
    "\n",
    "### Symmetric positive semi-definite matrices\n",
    "Equivalent definitions:\n",
    " 1. All eigenvalues are real and positive or zero ($\\lambda_i \\ge 0$)\n",
    " 2. Energy $x^T S x \\ge 0$ for any $x \\ne 0$\n",
    " 3. $S = A^T A$ (dependent columns allowed in $A$)\n",
    " 4. All leading determinants are $\\ge 0$\n",
    " 5. All pivots in elimination are $\\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 06: Singular Value Decomposition\n",
    "\n",
    "Like eigenvalues, but works for rectangular and singular matrices\n",
    "\n",
    "For a symmetric matrix, e'vals and e'vecs exist and are complete\n",
    "\n",
    "For general square matrix, not the case\n",
    "\n",
    "For rectangular matrix, certainly not\n",
    "\n",
    "The SVD of the $m \\times n$ matrix $A$ is $A = U \\Sigma V^T$, where $U$ is an $m \\times m$ orthogonal matrix, $V$ is an $n \\times n$ orthogonal matrix, and\n",
    "\n",
    "$\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_1 & 0        & \\ldots & 0 \\\\\n",
    "0        & \\sigma_2 & \\ldots & 0 \\\\\n",
    "0        & 0        & \\ddots & 0 \\\\\n",
    "\\end{bmatrix}$,\n",
    "\n",
    "where $s_i$ are the 'singular values'. $\\Sigma$ is $m \\times n$.  The $s_i$ are all positive.\n",
    "\n",
    "### The details\n",
    "\n",
    "1. The key observation is that $A^T A$ is symmetric, square, and positive semi-definite.\n",
    "2. Thus: $A^T A$ can be decomposed: $A^T A = V \\Lambda V^T$, with $V$ orthogonal, and $\\Lambda$ positive.\n",
    "3. We also have $A A^T$ is symmetric and positive semi-definite, and we have $A A^T = U \\Lambda U^T$\n",
    "4. Now look for $A v_i = \\sigma_i u_i$, where the $v_i$ and $u_i$ are sets of orthogonal vectors.\n",
    "\n",
    "\"We're looking for one set of orthogonal vectors in the 'input space' of $A$, and a set of orthogonal vectors in the 'output space' of $A$ that transform to each other via $A$\".\n",
    "\n",
    "We then have $A V = U \\Sigma$, which then gives us $A = U \\Sigma V^T$.\n",
    "\n",
    "Now... what are the $V$s and what are the $U$s.\n",
    "\n",
    "We can see that if $A = U \\Sigma V^T$ then $A^T A = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^2 V^T = V \\Lambda V^T$ --- that is, the $V$s are the eigenvectors of $A^T A$, and the $\\Lambda = \\Sigma^2$ are the eigenvalues of $A^T A$. Similarly for $A A^T$.\n",
    "\n",
    "Haven't quite finished: need to deal with the case of repeated eigenvalues --- and hence have 'eigenspaces'. Need to pick the appropriate eigenvectors from these spaces to satisfy $A v_i = \\sigma_i u_i$.\n",
    "\n",
    "We do this fixing the $v$s to be a particular set of eigenvectors of $A^T A$, and then solving $u_i = A v_i / \\sigma_i$ - this ensures that whatever choices we make for the $v_i$, we get the appropriate $u_i$ in the degenerate cases.\n",
    "\n",
    "Finally just need to show that the $u_i$'s picked in this way are orthogonal: $u_i^T u_i = \\frac{v_i^T A^T A v_j}{\\sigma_i \\sigma_j} = v_i^T v_j \\frac{\\sigma_j^2}{\\sigma_i \\sigma_j} = \\delta_{i,j}$\n",
    "\n",
    "### Geometry of the SVD\n",
    "\n",
    "\"Every matrix factors into a rotation, then 'stretch', then rotation\"\n",
    "\n",
    "![Geometry of the SVD](./images/06-01-svd_geometry.png)\n",
    "\n",
    "### The SVD as a sum of rank-1 matrices\n",
    "\n",
    "Can rewrite the SVD in the following form:\n",
    "\n",
    "$A = U \\Sigma V^T = \\sum_{i=1}^n \\sigma_i u_i v_i^T$\n",
    "\n",
    "where the singular values $\\sigma_i$ are in descending order (that is, $\\sigma_i \\ge \\sigma_j$ when $i \\le j$).\n",
    "\n",
    "Each of the $u_i v_i^T$ is a rank-1 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 07: Eckart-Young: The Closest Rank k Matrix to A\n",
    "\n",
    "### Eckart-Young theorem:\n",
    "\n",
    "The rank-$k$ approximation to $A$ you get by keeping only the $k$ highest singular values and their associated rank-1 matrices is in a sense the 'best' rank-$k$ approximation to $A$:\n",
    "\n",
    "Let $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$\n",
    "\n",
    "**Theorem**: Given any rank-$k$ matrix $B$, then $||A - B|| \\ge ||A - A_k||$. (NB: *though not for all norms - ?they must be orthogonally invariant?*)\n",
    "\n",
    "**Proof**: *Fill in*\n",
    "\n",
    "### Matrix norms\n",
    "\n",
    "Examples\n",
    "\n",
    "1. $L_2$ norm: $||A||_2 = \\sigma_1$, the largest singular value\n",
    "2. Frobenius norm: $||A||_F = \\sqrt{\\sum_{i,j} |A_{i,j}|^2} $\n",
    "3. Nuclear norm: $||A||_{n} = \\sum_i \\sigma_i$\n",
    "\n",
    "### Principal Component Analysis - PCA\n",
    "\n",
    "Give data $X$, find the best rank-$k$ approximation to the sample covariance matrix $\\frac{(X-\\bar{X})(X-\\bar{X})^T}{N-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 08: Norms of matrices and vectors\n",
    "\n",
    "A 'norm' is a way to measure the size of a thing.\n",
    "\n",
    "### Vector norms\n",
    "\n",
    "#### $||v||_p$\n",
    "\n",
    "$||v||_p = \\left(|v_1|^p + |v_2|^p + \\ldots \\right)^{1/p}$.\n",
    "\n",
    "What about p=0 (or really, $p \\lt 1$)?\n",
    "\n",
    "$||v||_0 = \\textrm{# of nonzero components}$ - but this is not a norm.\n",
    "\n",
    "$p$-balls of radius 1 in $R^2$:\n",
    "![p-balls of radius 1](./images/08-01-p-balls.png)\n",
    "\n",
    "#### $S$-norm\n",
    "\n",
    "Given a symmetric positive definite matrix $S$, then define $||v||_S = \\sqrt{v^T S v}$.\n",
    "\n",
    "This is a 'weighted norm', and the unit ball is an ellipse in $R^2$.\n",
    "\n",
    "#### A common problem class\n",
    "\n",
    "*Problem*: Minimise $||x||$, subject to the constraint that $A x = b$.\n",
    "\n",
    "When $||\\cdot||$ is the $l_1$ norm, this is called basis pursuit. The 'winning $x$' is sparse.\n",
    "\n",
    "When $||\\cdot||$ is the $l_2$ norm, this is least squares, or ridge regression. The 'winning $x$' is not sparse.\n",
    "\n",
    "### Matrix norms\n",
    "\n",
    "Have a matrix $A$.\n",
    "\n",
    "#### Getting a matrix norm from a vector norm\n",
    "\n",
    "In general, given a vector norm $||\\cdot||$, you can define a related matrix norm via\n",
    "\n",
    "$||A|| = \\max_x \\frac{||A x||}{||x||}$.\n",
    "\n",
    "**Proof**: *Prove that $||A||$ defined in this way is a valid norm*\n",
    "\n",
    "#### Example 1: $||A||_2 = \\sigma_1$\n",
    "\n",
    "This comes about by using the $l_2$ vector norm in the above.\n",
    "\n",
    "#### Example 2: The Frobenius norm $||A||_F = \\sqrt{\\sum |a_{ij}|^2}$\n",
    "\n",
    "**Proof**: *Prove how the Frobenius norm is related to the singular values*\n",
    "\n",
    "#### Example 3: The nuclear norm $||A||_n = \\sum |\\sigma_i|$\n",
    "\n",
    "Professor Srebro at U. Chicago has conjecture that deep learning  picks out the weight matrix that minimises the nuclear norm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 09: Four ways to solve Least Squares Problems\n",
    "\n",
    "1. Solve the normal equations $A^T A \\hat{x} = A^T b$\n",
    "2. Pseudoinverse of $m \\times n$ matrix $A$\n",
    "3. Orthogonalise first - Gram-Schmidt procedure\n",
    "4. (didn't get to this)\n",
    "\n",
    "### 1. Solve the normal equations\n",
    "\n",
    "Gauss's suggestion that when $A x = b$ has no solution, we should find the $\\hat{x}$ that minimises $||b - A \\hat{x}||_2$ leads to the *normal equations*:\n",
    "\n",
    "$A^T A \\hat{x} = A^T x$\n",
    "\n",
    "When $A$ has independent columns ($A$ is rank $n$), then \n",
    "$A^T A$ is invertible, and it's easy to solve $A^T A \\hat{x} = A^T b$ --- we just have $\\hat{x} = (A^T A)^{-1} A^T b$.\n",
    "\n",
    "### 2. Pseudoinverse $A^+$ of $A$\n",
    "\n",
    "Given $n \\times m$ matrix $A$, then the pseudomatrix $A^+$ is $n \\times m$, and is the matrix that is 'as close to the inverse of $A$ as possible' --- that is $A A^+$ is as close to $I$ as possible.\n",
    "\n",
    "The pseudoinverse inverts the action of $A$ on vectors in its row space (i.e. because that's the bit that *is* invertible):\n",
    "![Action of the Pseudoinverse](./images/09-01-pseudoinverse-action.png)\n",
    "\n",
    "**To do**: There are some properties of the four fundamental spaces of $A$ that I'd like to make sure I understand:\n",
    "\n",
    "1. I *think* that if $x \\in R(A)$ then $A x \\ne 0$.  Yes: this is true: $R^n = n(A) \\oplus c(A^T)$ -- I'm struggling to find an intuitive way to see this. The best I've come up with so far is to think about projections onto the null space or row space.\n",
    "\n",
    "The pseudoinverse of $A$ can easily be expressed relative to its SVD, $A = U \\Sigma V^T$. That is, $A^+ = V \\Sigma^+ U^T$, where the pseudoinverse $\\Sigma^+$ of $\\Sigma$ is the $n \\times m$ matrix:\n",
    "\n",
    "$\\Sigma^+ = \\begin{bmatrix}\n",
    "1/\\sigma_1 & 0          & 0      \\\\\n",
    "0          & 1/\\sigma_2 & 0      \\\\\n",
    "0          & 0          & \\ddots \\\\\n",
    "\\vdots     & \\vdots     & \n",
    "\\end{bmatrix}$.\n",
    "\n",
    "Connecting this back to the solutions of the normal equaions, consider the case that $A$ has independent columns (is rank $n$ or $N(A) = 0$). Then $A^T A$ is invertible, and we have $A^+ = (A^T A)^{-1} A^T$. *Check this*\n",
    "\n",
    "### 3. Gram-Schmidt\n",
    "\n",
    "This applies when $A$ has rank $n$ (just as for solving the normal equations).  Find an orthogonal basis for $C(A)$ by successively orthogonalising columns of $A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 10: Survey of difficulties with $A x = b$\n",
    "\n",
    "Have received the problem $A x = b$ (the key problem of linear algebra); have to produce an answer...\n",
    "\n",
    "0. Using the pseudoinverse: this always works $x = A^+ b$ - but may not be computationally feasible / stable\n",
    "1. Easy case: size of $A$ is ok, and $A$ is well conditioned, then $x = A \\backslash b$\n",
    "2. Too many equations: $m > n$, then $A^T A \\hat{x} = A^T b$ still works\n",
    "3. Underdetermined: $n < m$, then there are many solutions, and we need to have additional condition - e.g. minimise the $l^1$ norm. The key thing in ML applications is that the solution needs to generalise beyond the training data.\n",
    "4. Columns in bad condition (high correlation between them) - then Gram-Schmidt (orthogonalise the columns; $A = Q R$); Use column-pivoting to improve numerics.\n",
    "5. Near singular; have an 'inverse problem' - then add a penalty to regularise: $\\min{||A x - b||^2 + \\delta^2 ||x||^2}$ -- question is: how big to make the penalty $\\delta$ (**focus for this lecture**)\n",
    "6. Too big - i.e. $m$ and $n$ are really big; can't fit in memory. Then use iterative methods (like conjugate gradient or Krylov)\n",
    "7. Waaay too big - i.e. $m$ and $n$ are really, really big. Then 'randomized linear algebra': sample the columns of $A$, and work with the sample.\n",
    "\n",
    "Focussing on case 5.\n",
    "\n",
    "### Regularising $A x = b$ where $A$ is a near-singular matrix\n",
    "\n",
    "We solve for $\\hat{x}$ where $\\hat{x} = \\textrm{argmin}_x ||A x - b||^2 + \\delta^2 ||x||^2$ with $\\delta^2 \\gt 0$.  Equivalent to solving:\n",
    "\n",
    "$\\begin{bmatrix} A \\\\ \\delta I\\end{bmatrix} x = \\begin{bmatrix} b \\\\ 0\\end{bmatrix}$\n",
    "\n",
    "Natural question is what $\\delta$ should I choose? - won't answer in this lecture.\n",
    "\n",
    "Rather: what happens to $\\hat{x}$ in the limit as $\\delta \\rightarrow 0$?  We converge towards $x = A^+ b$ -- that is: for any $A$, $(A^T A + \\delta^2 I)^{-1} A^T$ converges to $A^+$ as $\\delta \\rightarrow 0$.\n",
    "\n",
    "To prove this, use the SVD. (\"If you want to prove anything about matrices, the SVD is your best tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 11: Gram-Schmidt\n",
    "\n",
    "The order in which you pick columns to orthogonalise can lead to numerical instability --- e.g. since if you pick a nearly co-linear column to orthogonalise, then the residual can be very small.\n",
    "\n",
    "- Once you get $q_1$ set, then need to compare $q_1$ against all the other columns of $A$, and pick the one that gives the largest residual; that is, the column of $A$ that is closest to orthogonal to $q_1$.\n",
    "- No extra work is involved in performing the Gram-Schmidt decomposition, as you can re-use the calculations made in selecting the next pivot column.\n",
    "\n",
    "### Krylov\n",
    "\n",
    "Have a large, sparse $A$.  Then can very cheaply multiply $A$ by a vector $b$.\n",
    "\n",
    "Solving $A x = b$:\n",
    "\n",
    "1. Create the vectors: $b$, $A b$, $A (A b)$, ..., $A^{j-1} b$ --- the span of these vectors is the \"Krylov space\", $K_j$\n",
    "2. Now select the solution $x_j$, the vector closest to $b$ in $K_j$.\n",
    "\n",
    "The one issue: some of the vectors spanning $K$ (i.e. those calculated in step 1) may be close to linearly dependent --- thus orthogonalise to find a good basis for $K$ to work with.\n",
    "\n",
    "Lanczos algorithm --- a good way to find an orthogonal basis for $K$: it's just Gram-Schmidt applied to the basis for $K_j$ found in 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 12: Computing Eigenvalues and Singular Values\n",
    "\n",
    "Key point: find transformations that preserve the e'vals or singular values, and use these to get matrices into a nice form for computation.\n",
    "\n",
    "### Eigenvalues\n",
    "#### The $Q R$ method\n",
    "\n",
    "Have a matrix $A = Q R$, with $Q$ an orthogonal basis for $C(A)$.\n",
    "\n",
    "Iterate the following:\n",
    "\n",
    "1. $A_i = Q_i R_i$; define $A_{i+1} = R_i Q_i$.  Note that $A_{i+1}$ will have the same e'vals as $A_i$ (since it's similar)\n",
    "2. Decompose $A_{i+1} = Q_{i+1} R_{i+1}$\n",
    "\n",
    "Most of the time, $j$ increases, the off-diagonal terms in $A_{j}$ get smaller and smaller, and the diagonal terms converge to the eigenvalues of $A_0$.\n",
    "\n",
    "**Question**: What happens for matrices with complex e'vals?\n",
    "\n",
    "#### Introduce 'shifts' to the $Q R$ method\n",
    "\n",
    "Start with the 'shifted' $A_0 - s I$: The eigenvectors are the same, the eigenvalues are reduced by $s$.\n",
    "\n",
    "1. Take $A_i$; shift it and decompose $A_i - s_i I = Q_i R_i$\n",
    "2. Get $A_{i+1} = R_i Q_i + s_i I$ --- then $A_{i+1}$ is similar to $A_i$ and thus has the same e'vals, but the shifts speed up the convergence.\n",
    "\n",
    "#### Transform $A$ to Hessenberg form before beginning\n",
    "\n",
    "The computational work in the previous method is all in finding the $Q R$ decomposition at each step.\n",
    "\n",
    "It turns out that it's easy to get $A$ to \"Upper Hessenberg\" form via a similarity transformation with simple calculations (almost upper triangular, but with one subdiagonal).\n",
    "\n",
    "It also turns out that $A_j$ remain in Hessenberg form as we go.\n",
    "\n",
    "On top of that, if $A$ is symmetric, then the Hessenberg form is tridiagonal, which is really nice to work with.\n",
    "\n",
    "### Singular values\n",
    "\n",
    "The singular values of $A$ are invariant to multiplication from the left or right by orthogonal matrices. Can pick whatever orthogonal matrices you like on either side --- thus have more freedom to simplify $A$.  Can then easily get $A$ to (upper) bidiagonal form without changing the singular values. Then find the eigenvalues of $A^T A$ via the methods above.\n",
    "\n",
    "The convergence of the above is $O(n^3)$ --- so these techniques work really well up to some size $n$. For matrices bigger than that, can use Krylov's method.\n",
    "\n",
    "**Observation**: I wonder whether there's any connection between Krylov and Takens' embedding theorem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 13: Randomised matrix multiplication\n",
    "\n",
    "This is an approach for approximately multiplying matrices together by sampling rows and columns from them, and using the product of those samples as an estimate:\n",
    "\n",
    "Given\n",
    "$A B = \\begin{bmatrix}a_1 & a_2 & \\ldots & a_r \\end{bmatrix} \\begin{bmatrix}b_1^T \\\\ b_2^T \\\\ \\vdots \\\\ b_r^T \\end{bmatrix} = a_1 b_1^T + \\ldots + a_r b_r^T$,\n",
    "then the idea is to choose columns/rows randomly (with some probability distribution to be determined), and approximate the complete product with a weighted sum of the products of these randomly selected columns/rows.\n",
    "\n",
    "Then we have that:\n",
    "\n",
    "- the entries in the resulting distribution have a mean equal to the entries in the exact $A B$\n",
    "- we'd like to understand the variance --- it's not zero...\n",
    "- we'd like to find the probability distribution by which we'll pick columns/rows so that the variance is minimised.\n",
    "- Can do this using lagrange multipliers\n",
    "\n",
    "**do the relevant computations for $A B$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 14: Low rank changes in $A$ and its inverse\n",
    "\n",
    "### Matrix inversion formula (signal processing)\n",
    "\n",
    "#### For the identity, with rank-1 perturbation\n",
    "Start with the identity, $I$, and perturb it by a rank-one matrix, and find the inverse:\n",
    "\n",
    "$(I - u v^T)^{-1} = I + \\frac{u v^T}{1 - v^T u}$\n",
    "\n",
    "i.e. we find the inverse of an $n \\times n$ matrix in terms of a $1 \\times 1$ matrix.\n",
    "\n",
    "Where we're going with this: if you perturb any matrix by a rank 1 matrix, then the change in the inverse is also rank 1.\n",
    "\n",
    "***TODO: Check that this is the inverse***\n",
    "\n",
    "#### For the identity with rank $k$ perturbation\n",
    "\n",
    "$(I_n - U V^T)^{-1} = I + U(I_k - V^T U)^{-1} V^T$\n",
    "where $U$ is $n\\times k$ and $V^T$ is $k \\times n$.\n",
    "\n",
    "i.e. you find the inverse of an $n \\times n$ matrix by finding the inverse of a $k \\times k$ matrix.\n",
    "\n",
    "#### Sherman-Morrison-Woodbury: for a general $A$\n",
    "\n",
    "$(A - U V^T)^{-1} = A^{-1} - A^{-1} U (I - V^T A^{-1} U)^{-1} V^T A^{-1}$\n",
    "\n",
    "$A$ = $n \\times n$, $U$ = $n \\times k$, $V^T$ = $k \\times n$.\n",
    "\n",
    "#### How would you use this fact?\n",
    "\n",
    "**1. Quickly solving a related linear system**\n",
    "\n",
    "Quickly solving $(A - U V^T) x = b$, when we know the solution to $A w = b$.\n",
    "\n",
    "1. solve $A z = u$ (this can be done quickly, since decomposing $A$ is the hard part)\n",
    "2. we now have $x = w + \\frac{w v^T z}{1 - v^T z}$.\n",
    "\n",
    "**2. Recursive least squares**\n",
    "\n",
    "New measurement in least squares: i.e. you start with $A x = b$, and solve the normal equations $A^T A x = A^T b$.\n",
    "\n",
    "Now get a new measurement (one more row in $A$, one more entry in $b$). Then the new normal equation is $(A^T A + v v^T) x_{new} = A^T b + v b_{new}$.\n",
    "\n",
    "i.e. we have a rank-1 change (or rank $k$ change if $k$ new measurements) in $A^T A$: \"Recursive least squares\". \n",
    "\n",
    "Related to Kalman filter: \"Dynamic least squares\" --- extends this in two ways: (1) weighted least squares using covariance matrix, and (2) state equation that describes dynamics of the system.\n",
    "\n",
    "\n",
    "#### How would this have been discovered?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 15: Matrices $A(t)$ depending on $t$, Derivative $dA/dt$\n",
    "\n",
    "Given we know $dA/dt$, what is $d A^{-1}/dt$?\n",
    "\n",
    "Helpful identity: $B^{-1} - A^{-1} = B^{-1}(A-B)A^{-1}$.\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "$\\frac{d A^{-1}}{d t} = \\lim_{\\epsilon \\rightarrow 0} \\frac{A(t+\\epsilon)^{-1} - A(t)^{-1}}{\\epsilon} = - \\lim_{\\epsilon \\rightarrow 0}A(t + \\epsilon)^{-1} \\frac{A(t + \\epsilon) - A(t)}{d t} A(t)^{-1} \\\\ = - A^{-1} \\frac{d A}{d t} A^{-1}$\n",
    "\n",
    "### Derivative of $\\lambda(t)$\n",
    "\n",
    "Facts:\n",
    "\n",
    "1. $A(t) x(t) = \\lambda(t) x(t)$\n",
    "2. The eigenvalues of $A^T$ are the same as $A$; the e'vecs are not: $y^T(t) A(t) = \\lambda(t) y^T(t)$\n",
    "3. Normalise the e'vecs: $y^T x = 1$\n",
    "\n",
    "In matrix notation:\n",
    "\n",
    "$A(t) X(t) = X(t) \\Lambda(t)$ and $Y^T(t) A(t) = \\Lambda(t) Y^T(t)$.\n",
    "\n",
    "We have $y^T(t) A(t) x(t) = \\lambda(t) y^T(t) x(t) = \\lambda(t)$\n",
    "\n",
    "Now: Formula for the time-derivative of $\\lambda$:\n",
    "\n",
    "$\\frac{d \\lambda}{d t} = \\frac{d}{d t} y^T(t) A(t) x(t) \\\\ = \\frac{d y^T}{d t} A x + y^T \\frac{d A}{d t} x + y^T A \\frac{d x}{d t} = \\lambda (\\frac{d y^T}{d t} x + y^T \\frac{d x}{d t}) + y^T \\frac{d A}{d t} x \\\\ = \\lambda \\frac{d}{d t} y^T x + y^T \\frac{d A}{d t} x = y^T \\frac{d A}{d t} x$\n",
    "\n",
    "### Change in e'vals, given a rank-1 change?\n",
    "\n",
    "Symmetric $S$ goes to $S + u u^T$, and let $\\gamma_j$ be the (descending-ordered) eigenvalues of $S$.\n",
    "\n",
    "What can we say about the eigenvalues $\\lambda_i$ of $S + u u^T$?\n",
    "\n",
    "It turns out that the e'vals \"interlace\":\n",
    "\n",
    "$\\lambda_1 \\ge \\gamma_1 \\ge \\lambda_2 \\ge \\gamma_2 \\ge \\ldots$\n",
    "\n",
    "**Puzzle**: Suppose $u$ is the $\\gamma_2$ eigenvector of $A$, then we have: $(A + \\alpha u u^T) u = (\\gamma_2 + \\alpha ||u||^2) u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 16\n",
    "\n",
    "*placeholder*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 17: Rapidly decreasing singular values\n",
    "\n",
    "(Alex Townshend)\n",
    "\n",
    "\"Why are there so many low-rank matrices in the world?\"\n",
    "\n",
    "$X$ = $n \\times n$ real matrix\n",
    "\n",
    "singular values: $\\sigma_1(X) \\ge \\sigma_2(x) \\ge \\ldots \\ge \\sigma_n(X) \\ge 0$\n",
    "\n",
    "**Fact:** If $\\sigma_{k+1}(X) = 0$ and $\\sigma_k(x) \\gt 0$ (i.e. there are $k$ nonzero singular values), then:\n",
    "\n",
    "1. $rank(X) = k$\n",
    "2. $X = u_1 v_1^T + \\ldots + u_k v_k^T$\n",
    "3. $dim(C(X)) = dim(R(X)) = k$\n",
    "\n",
    "**Definition:** $X$ is low rank if $2 k n \\lt n^2$ --- i.e. we can communicate $X$ more efficiently by sending its decomposition into rank-1 matrices, rather than sending the $n^2$ components of $X$.\n",
    "\n",
    "(And often we demand that $k \\lt\\lt n/2$.\n",
    "\n",
    "### What do low rank matrices look like?\n",
    "\n",
    "rank-1: matrix is 'highly aligned'\n",
    "\n",
    "Triangular matrices are not low rank\n",
    "\n",
    "\"Circular\" matrices are low rank: lots of symmetry\n",
    "![Japanese flag is low rank](./images/17-japan-flag-is-low-rank.png)\n",
    "\n",
    "### Numerical rank\n",
    "\n",
    "\"wiggle room\" around rank:\n",
    "\n",
    "For $0 \\lt \\epsilon \\lt 1$ ($\\epsilon$ is the tolerance), then $rank_\\epsilon(X) = k$ when $k$ is the last singular value above $\\epsilon$.\n",
    "\n",
    "1. $\\sigma_{k+1}(X) \\le \\epsilon \\sigma_1(X)$ and\n",
    "2. $\\sigma_k(X) \\gt \\epsilon \\sigma_1(X)$\n",
    "\n",
    "(and $rank_0(X) = rank(X)$)\n",
    "\n",
    "Eckart-Young: \"singular values tell us how well we can approximate $X$ by a rank $k$ matrix\", i.e. $\\sigma_{k+1}(X) = ||X - X_k||$, where $X_k$ is the 'best rank-$k$ approximation' of $X$\n",
    "\n",
    "On a computer, bounded by machine precision, so machine can't tell the difference for $\\epsilon$ less than floating point precision.\n",
    "\n",
    "### Matrices with low numerical rank\n",
    "\n",
    "1. Hilbert matrix: $H_{jk} = \\frac{1}{j + k - 1}$ -- low numerical rank, but complete rank.\n",
    "2. Vandermonde matrix: $V = \\begin{bmatrix} 1 & X_1 & \\ldots & X_1^{n-1} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & X_n & \\ldots & X_n^{n-1} \\end{bmatrix}$, $x_i$ real -- nasty matrix, as low numerical rank, but often want to invert.\n",
    "\n",
    "Why are matrices low rank? \"Because the world is smooth\"\n",
    "\n",
    "### The world is smooth\n",
    "\n",
    "(John Reade, 1983)\n",
    "\n",
    "Example: $p(x, y) = 1 + x + x y$, and $X_{jk} = P(j, k) = 1 + j + j k$, then $X$ is low rank (in fact, rank 3).\n",
    "\n",
    "In general: given a polynomial in two variables: $p(x, y) = \\sum_{s=0}^{m-1} \\sum_{t=0}^{m-1} a_{st} x^s y^t$, and sample it: $X_{jk} = p(j,k)$, then $rank(X) \\lt m^2$.\n",
    "\n",
    "More generally again: sampling a smooth function leads to low numerical rank, as the function can be well approximated by a polynomial.\n",
    "\n",
    "Unfortunately the reasoning doesn't work too well for bounding the rank (i.e. Hilbert matrix with 1000 by 1000 entries with $\\epsilon = 10^{-15}$, Reade's approach gives numerical rank of about 719, but actually it's 28)\n",
    "\n",
    "### The world is Sylvester\n",
    "(Alex Townshend)\n",
    "\n",
    "That is, real world matrices satisfy a Sylvester equation: $A X - X B = C$.\n",
    "\n",
    "For example:\n",
    "\n",
    "1. Hilbert matrix: satisfies $diag(1/2, 3/2, \\ldots, n-1/2) H - H diag(-1/2, -3/2, \\ldots, -n + 1/2) = (1) (1)^T$\n",
    "2. Vandermonde: $diag(x_1, \\ldots, x_n) V - V A = $\n",
    "\n",
    "**Theorem**: If $X$ satisfies $A X - X B = C$ (and $A$ and $B$ are normal), then $\\sigma_{r k+1}(X) \\le Z_k(E, F) \\sigma_1(X)$, with $rank(C) = r$, and $Z_k$ the \"Zolotarev number, with $E$ the set of e'vals of $A$ and $F$ the set of e'vals of $B$.\n",
    "\n",
    "Key thing for the examples above: $E$ and $F$ are separated -- and when they're separated, then $Z$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 18: Counting parameters in SVD, LU, QR, Saddle Points\n",
    "\n",
    "### Number of free parameters in various matrices / matrix decompositions\n",
    "Checking that the number of free parameters in each of the factorisations agrees with the number of free parameters in the matrix itself.\n",
    "\n",
    "For square matrices:\n",
    "\n",
    "- $L$: $n (n-1)/2$ parameters (diagonal are all $1$s)\n",
    "- $U$: $n (n+1)/2$ parameters\n",
    "- $Q$: orthogonal matrix has $n-1 + n-2 + n-3 + \\ldots + 1 = n(n-1)/2$\n",
    "- $\\Lambda$: $n$ parameters\n",
    "- $X$: eigenvector matrix has $n^2 - n$ parameters\n",
    "- $S$: symmetric matrix has $n + n(n-1)/2 = n(n+1)/2$\n",
    "\n",
    "So:\n",
    "\n",
    "- $A = L U$: $n(n-1)/2 + n(n+1)/2 = n^2$\n",
    "- $A = Q R = n(n-1)/2 + n(n+1)/2 = n^2$\n",
    "- $S = Q \\Lambda Q^{-1} = n(n-1)/2 + n = n(n+1)/2$\n",
    "- SVD for full-rank $n\\times m$ matrix $A$: $A = U \\Sigma V^T = m(m-1)/2 + m + (n-1) + (n-2) + \\ldots + (n-m) = m(m-1)/2 + m + n m - m(m+1)/2 = n m$ (we only want the columns of $V$ corresponding to the singular values).\n",
    "- SVD for partial-rank $n \\times m$ matrix $A$: $A$ would have $m-1 + m-2 + \\ldots + m-r + r + n-1 + n-2 + \\ldots + n-r = (m + n)r - r^2$ --- that is: a general rank $r$ matrix has $(m+n)r - r^2$ parameters.\n",
    "\n",
    "### Saddle points\n",
    "\n",
    "2 main sources of saddle points:\n",
    "\n",
    "#### arising from a constraint: i.e. coming from lagrange multiplier\n",
    "\n",
    "Looking to minimise $\\frac{1}{2} x^T S x$, with $S$ symmetric positive definite, subject to constraint $A x = b$.\n",
    "\n",
    "Lagrange multipliers give us:\n",
    "$L(x, \\lambda) = \\frac{1}{2} x^T S x + \\lambda^T (A x-b)$\n",
    "\n",
    "Differentiation gives:\n",
    "\n",
    "$S x - A^T \\lambda = 0$, and\n",
    "\n",
    "$A x = b$\n",
    "\n",
    "In block matrix form this gives:\n",
    "\n",
    "$\\begin{bmatrix}S & A^T \\\\ A & 0\\end{bmatrix}\\begin{bmatrix}x \\\\ \\lambda\\end{bmatrix} = \\begin{bmatrix}0 \\\\ b\\end{bmatrix}$\n",
    "\n",
    "The coefficient matrix is positive semidefinite (has some zero e'vals).\n",
    "\n",
    "This is the \"KKT\" matrix; the KKT conditions.\n",
    "\n",
    "Doing elimination on the KKT matrix:\n",
    "\n",
    "$\\begin{bmatrix}S & A^T \\\\ A & 0\\end{bmatrix} \\rightarrow \\begin{bmatrix}S & A^T \\\\ 0 & - A S^{-1} A^T\\end{bmatrix}$, and $-A S^{-1} A^T$ is *negative definite*, and so we have $m$ positive and $n$ negative pivots.  And the signs of the pivots in elimination give us the signs of the eigenvalues: which tells us we have a a saddle point.\n",
    "\n",
    "#### Saddles arising from $R(x) = \\frac{x^T S x}{x^T x}$\n",
    "\n",
    "The Rayleigh quotient ($R(x)$) has a maximum value of $\\lambda_1$, occurring at $x = c q_1$, some multiple of the major eigenvector.\n",
    "\n",
    "Similarly, has a minimum value of $\\lambda_n$, with $x = c q_n$.\n",
    "\n",
    "For $x$ any other eigenvector, we have a saddlepoint. (continued in next lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 19: Saddle points continued, Maxmin principle\n",
    "\n",
    "\"Saddle points are a 'maximum of a minimum'\", and this also leads to the interlacing theorem.\n",
    "\n",
    "(Example applied to Rayleigh quotient of $diag(5, 3, 1)$)\n",
    "\n",
    "Vandermonde matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
