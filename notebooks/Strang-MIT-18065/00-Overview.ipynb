{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Gilbert Strang's 2018 Matrix Methods course\n",
    "\n",
    "- Linear algebra => Optimisation => Deep learning\n",
    "- Linear Algebra => Statistics => Deep learning\n",
    "- [\"Learning from data\" book](math.mit.edu/learningfromdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 01: The column space of $A$ contains all vectors $A x$\n",
    "\n",
    "- Think of the product $A x$ as a linear combination of the columns of $A$: $A x = A_{:,1} x_1 + A_{:,2} x_2 + \\ldots + A_{:,n} x_n$\n",
    "- $A = C R$ where the columns of $C$ form a basis for $C(A)$, and each column in $C$ is a column of $A$; then $R$ is the first $rank(A)$ rows of (a column-permutation of) $rref(A)$.\n",
    "- Given $C$, a matrix formed from $r = rank(A)$ l.i. columns of $A$, and $R$, a matrix formed from $r$ l.i. rows of $A$, then there is a matrix $U$ such that $A = C U R$, and $U$ is an $r \\times r$ invertible matrix\n",
    "  - Question: Are there any more properties of $U$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 02: Multiplying and factoring matrices\n",
    "\n",
    "### 5 key factorisations\n",
    "\n",
    "- $A = L U$ -- Elimination\n",
    "- $A = Q R$ -- Gram-Schmidt decomposition\n",
    "- $S = Q \\Lambda Q^T$ -- Spectral theorem (for symmetric matrices $S$)\n",
    "- $A = X \\Lambda X^{-1}$ -- Doesn't work for all matrices\n",
    "- $A = U \\Sigma V^T$ -- Singular Value Decomposition; works for all matrices; orthogonal * diagonal * orthogonal\n",
    "\n",
    "### LU decomposition in rank-1 picture\n",
    "\n",
    "- $A = l_1 u_1^T + \\begin{pmatrix}0 & 0 \\\\ 0 & l_2 u_2^T \\end{pmatrix} + \\begin{pmatrix}0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & l_3 u_3^T \\end{pmatrix} + \\ldots $\n",
    "\n",
    "### Orthogonality of fundamental spaces of a matrix $A$\n",
    "\n",
    "- $C(A^T)$ is orthogonal to $N(A)$ \n",
    "  - i.e. because each row in $A$ is orthogonal to any vector in $N(A)$\n",
    "- $C(A)$ is orthogonal to $N(A^T)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 03: Orthonormal columns in $Q$ give $Q^T Q = I$\n",
    "\n",
    "$Q$ is used to denote a matrix with orthonormal columns - that is, $q_{:,i}^T q_{:,j} = \\delta_{i,j}$.\n",
    "\n",
    "Thus:\n",
    "- $Q^T Q = I_m$, and\n",
    "- $Q Q^T = \\begin{pmatrix}I_n & 0 \\\\ 0 & 0\\end{pmatrix}$\n",
    "\n",
    "If $Q^T Q = Q Q^T = I$, then $Q$ is 'orthogonal'.\n",
    "\n",
    "### Orthogonal matrices preserve length under $l_2$\n",
    "\n",
    "i.e. $|Q x| = |x|$\n",
    "\n",
    "**proof**: $|Q x|^2 = |(Q x)^T (Q x)| = |x^T (Q^T Q) x| = |x^T x| = |x|^2$\n",
    "\n",
    "### Examples of orthogonal matrices\n",
    "\n",
    "#### rotation matrices\n",
    "$\\begin{pmatrix} cos{\\theta} & sin{\\theta} \\\\ -sin{\\theta} & cos{\\theta}\\end{pmatrix}$\n",
    "\n",
    "Rotates anti-clockwise by $\\theta$ around the origin in 2-d\n",
    "  \n",
    "#### reflection matrices\n",
    "$\\begin{pmatrix} cos{\\theta} & sin{\\theta} \\\\ sin{\\theta} & -cos{\\theta}\\end{pmatrix}$\n",
    "\n",
    "Reflects plane in the line at $\\theta/2$\n",
    "\n",
    "#### \"Householder reflections\"\n",
    "Given unit vector $u$, then $H = I - 2 u u^T$\n",
    "\n",
    "#### \"Hadamard\" matrices\n",
    "\n",
    "$H_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$\n",
    "\n",
    "$H_{2^n} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} H_{2^{n-1}} & H_{2^{n-1}} \\\\ H_{2^{n-1}} & -H_{2^{n-1}} \\end{pmatrix}$\n",
    "\n",
    "**Conjecture**: There is an orthogonal matrix of size $n \\times n$ with entries $1$ and $-1$ for $n$ a multiple of $4$ --- known up to $n=668$.\n",
    "\n",
    "#### Wavelets\n",
    "\n",
    "$W_4 = \\begin{pmatrix}\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 &-1 & 0 \\\\\n",
    "1 &-1 & 0 & 1 \\\\\n",
    "1 &-1 & 0 &-1\n",
    "\\end{pmatrix}$\n",
    "\n",
    "(with some scaling on the columns to make them orthonormal)\n",
    "\n",
    "Haar invented in 1910; Ingrid Daubechies 1988 - found families of wavelets with entries that were not just 1 and -1.\n",
    "\n",
    "#### Eigenvectors of a symmetric matrix\n",
    "\n",
    "Example: discrete fourier transform is the matrix of eigenvectors of $Q^T Q$, with $Q = P_{2,3,\\ldots,n-1,n,1}$ (i.e. $Q$ is the permutation matrix that puts row 2 in row 1, row 3 in row 2, etc.)\n",
    "\n",
    "*I didn't understand this bit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 04: Eigenvalues and Eigenvectors\n",
    "\n",
    "Useful because they allow you to work with powers of matrices.\n",
    "\n",
    "The eigenvectors of a general matrix $A$ are not necessarily orthogonal to each other. (*Find an example*)\n",
    "\n",
    "### Similar matrices\n",
    "**Definition**: $B$ is *similar* to $A$ if there exists a matrix $M$ such that $B = M^{-1} A M$.\n",
    "\n",
    "If $B$ is similar to $A$, then they have the same eigenvalues. (Easy to prove)\n",
    "\n",
    "Corollary: (invertible) $A B$ and $B A$ have the same non-zero eigenvalues.  (Just use $M = B$).\n",
    "\n",
    "Computing eigenvalues of $A$ --- usually involves picking better and better values of $M$ to find a triangular matrix similar to $A$. (*How does this work?*)\n",
    "\n",
    "### (Real) Symmetric matrices\n",
    "- Have real eigenvalues (*prove this*)\n",
    "- Have orthogonal eigenvectors (*prove this*)\n",
    "- And thus $Q$, the matrix with eigenvectors as columns is orthonormal, and\n",
    "- $S = Q \\Lambda Q^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 05: Positive definite and semidefinite matrices\n",
    "\n",
    "### Symmetric positive definite matrices\n",
    "Equivalent definitions:\n",
    " 1. All eigenvalues are real and positive ($\\lambda_i > 0$)\n",
    " 2. Energy $x^T S x > 0$ for any $x \\ne 0$\n",
    " 3. $S = A^T A$ (independent columns in $A$)\n",
    " 4. All leading determinants are > 0\n",
    " 5. All pivots in elimination are > 0\n",
    " \n",
    "**Example:**\n",
    "$S = \\begin{pmatrix}3 & 4 \\\\ 4 & 5\\end{pmatrix}$ is *not* positive definite (its determinant is $|A| = 15 - 16 = -1$).\n",
    "\n",
    "$x^T S x$ is a quadratic form -- if $S$ is positive definite, then $S$ is convex with a unique minimum.\n",
    "\n",
    "### Symmetric positive semi-definite matrices\n",
    "Equivalent definitions:\n",
    " 1. All eigenvalues are real and positive or zero ($\\lambda_i \\ge 0$)\n",
    " 2. Energy $x^T S x \\ge 0$ for any $x \\ne 0$\n",
    " 3. $S = A^T A$ (dependent columns allowed in $A$)\n",
    " 4. All leading determinants are $\\ge 0$\n",
    " 5. All pivots in elimination are $\\ge 0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
