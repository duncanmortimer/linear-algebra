{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Gilbert Strang's 2018 Matrix Methods course\n",
    "\n",
    "- Linear algebra => Optimisation => Deep learning\n",
    "- Linear Algebra => Statistics => Deep learning\n",
    "- [\"Learning from data\" book](math.mit.edu/learningfromdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 01: The column space of $A$ contains all vectors $A x$\n",
    "\n",
    "- Think of the product $A x$ as a linear combination of the columns of $A$: $A x = A_{:,1} x_1 + A_{:,2} x_2 + \\ldots + A_{:,n} x_n$\n",
    "- $A = C R$ where the columns of $C$ form a basis for $C(A)$, and each column in $C$ is a column of $A$; then $R$ is the first $rank(A)$ rows of (a column-permutation of) $rref(A)$.\n",
    "- Given $C$, a matrix formed from $r = rank(A)$ l.i. columns of $A$, and $R$, a matrix formed from $r$ l.i. rows of $A$, then there is a matrix $U$ such that $A = C U R$, and $U$ is an $r \\times r$ invertible matrix\n",
    "  - Question: Are there any more properties of $U$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 02: Multiplying and factoring matrices\n",
    "\n",
    "### 5 key factorisations\n",
    "\n",
    "- $A = L U$ -- Elimination\n",
    "- $A = Q R$ -- Gram-Schmidt decomposition\n",
    "- $S = Q \\Lambda Q^T$ -- Spectral theorem (for symmetric matrices $S$)\n",
    "- $A = X \\Lambda X^{-1}$ -- Doesn't work for all matrices\n",
    "- $A = U \\Sigma V^T$ -- Singular Value Decomposition; works for all matrices; orthogonal * diagonal * orthogonal\n",
    "\n",
    "### LU decomposition in rank-1 picture\n",
    "\n",
    "- $A = l_1 u_1^T + \\begin{pmatrix}0 & 0 \\\\ 0 & l_2 u_2^T \\end{pmatrix} + \\begin{pmatrix}0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & l_3 u_3^T \\end{pmatrix} + \\ldots $\n",
    "\n",
    "### Orthogonality of fundamental spaces of a matrix $A$\n",
    "\n",
    "- $C(A^T)$ is orthogonal to $N(A)$ \n",
    "  - i.e. because each row in $A$ is orthogonal to any vector in $N(A)$\n",
    "- $C(A)$ is orthogonal to $N(A^T)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 03: Orthonormal columns in $Q$ give $Q^T Q = I$\n",
    "\n",
    "$Q$ is used to denote a matrix with orthonormal columns - that is, $q_{:,i}^T q_{:,j} = \\delta_{i,j}$.\n",
    "\n",
    "Thus:\n",
    "- $Q^T Q = I_m$, and\n",
    "- $Q Q^T = \\begin{pmatrix}I_n & 0 \\\\ 0 & 0\\end{pmatrix}$\n",
    "\n",
    "If $Q^T Q = Q Q^T = I$, then $Q$ is 'orthogonal'.\n",
    "\n",
    "### Orthogonal matrices preserve length under $l_2$\n",
    "\n",
    "i.e. $|Q x| = |x|$\n",
    "\n",
    "**proof**: $|Q x|^2 = |(Q x)^T (Q x)| = |x^T (Q^T Q) x| = |x^T x| = |x|^2$\n",
    "\n",
    "### Examples of orthogonal matrices\n",
    "\n",
    "#### rotation matrices\n",
    "$\\begin{pmatrix} cos{\\theta} & sin{\\theta} \\\\ -sin{\\theta} & cos{\\theta}\\end{pmatrix}$\n",
    "\n",
    "Rotates anti-clockwise by $\\theta$ around the origin in 2-d\n",
    "  \n",
    "#### reflection matrices\n",
    "$\\begin{pmatrix} cos{\\theta} & sin{\\theta} \\\\ sin{\\theta} & -cos{\\theta}\\end{pmatrix}$\n",
    "\n",
    "Reflects plane in the line at $\\theta/2$\n",
    "\n",
    "#### \"Householder reflections\"\n",
    "Given unit vector $u$, then $H = I - 2 u u^T$\n",
    "\n",
    "#### \"Hadamard\" matrices\n",
    "\n",
    "$H_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$\n",
    "\n",
    "$H_{2^n} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} H_{2^{n-1}} & H_{2^{n-1}} \\\\ H_{2^{n-1}} & -H_{2^{n-1}} \\end{pmatrix}$\n",
    "\n",
    "**Conjecture**: There is an orthogonal matrix of size $n \\times n$ with entries $1$ and $-1$ for $n$ a multiple of $4$ --- known up to $n=668$.\n",
    "\n",
    "#### Wavelets\n",
    "\n",
    "$W_4 = \\begin{pmatrix}\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 &-1 & 0 \\\\\n",
    "1 &-1 & 0 & 1 \\\\\n",
    "1 &-1 & 0 &-1\n",
    "\\end{pmatrix}$\n",
    "\n",
    "(with some scaling on the columns to make them orthonormal)\n",
    "\n",
    "Haar invented in 1910; Ingrid Daubechies 1988 - found families of wavelets with entries that were not just 1 and -1.\n",
    "\n",
    "#### Eigenvectors of a symmetric matrix\n",
    "\n",
    "Example: discrete fourier transform is the matrix of eigenvectors of $Q^T Q$, with $Q = P_{2,3,\\ldots,n-1,n,1}$ (i.e. $Q$ is the permutation matrix that puts row 2 in row 1, row 3 in row 2, etc.)\n",
    "\n",
    "*I didn't understand this bit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 04: Eigenvalues and Eigenvectors\n",
    "\n",
    "Useful because they allow you to work with powers of matrices.\n",
    "\n",
    "The eigenvectors of a general matrix $A$ are not necessarily orthogonal to each other. (*Find an example*)\n",
    "\n",
    "### Similar matrices\n",
    "**Definition**: $B$ is *similar* to $A$ if there exists a matrix $M$ such that $B = M^{-1} A M$.\n",
    "\n",
    "If $B$ is similar to $A$, then they have the same eigenvalues. (Easy to prove)\n",
    "\n",
    "Corollary: (invertible) $A B$ and $B A$ have the same non-zero eigenvalues.  (Just use $M = B$).\n",
    "\n",
    "Computing eigenvalues of $A$ --- usually involves picking better and better values of $M$ to find a triangular matrix similar to $A$. (*How does this work?*)\n",
    "\n",
    "### (Real) Symmetric matrices\n",
    "- Have real eigenvalues (*prove this*)\n",
    "- Have orthogonal eigenvectors (*prove this*)\n",
    "- And thus $Q$, the matrix with eigenvectors as columns is orthonormal, and\n",
    "- $S = Q \\Lambda Q^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 05: Positive definite and semidefinite matrices\n",
    "\n",
    "### Symmetric positive definite matrices\n",
    "Equivalent definitions:\n",
    " 1. All eigenvalues are real and positive ($\\lambda_i > 0$)\n",
    " 2. Energy $x^T S x > 0$ for any $x \\ne 0$\n",
    " 3. $S = A^T A$ (independent columns in $A$)\n",
    " 4. All leading determinants are > 0\n",
    " 5. All pivots in elimination are > 0\n",
    " \n",
    "**Example:**\n",
    "$S = \\begin{pmatrix}3 & 4 \\\\ 4 & 5\\end{pmatrix}$ is *not* positive definite (its determinant is $|A| = 15 - 16 = -1$).\n",
    "\n",
    "$x^T S x$ is a quadratic form -- if $S$ is positive definite, then $S$ is convex with a unique minimum.\n",
    "\n",
    "### Symmetric positive semi-definite matrices\n",
    "Equivalent definitions:\n",
    " 1. All eigenvalues are real and positive or zero ($\\lambda_i \\ge 0$)\n",
    " 2. Energy $x^T S x \\ge 0$ for any $x \\ne 0$\n",
    " 3. $S = A^T A$ (dependent columns allowed in $A$)\n",
    " 4. All leading determinants are $\\ge 0$\n",
    " 5. All pivots in elimination are $\\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 06: Singular Value Decomposition\n",
    "\n",
    "Like eigenvalues, but works for rectangular and singular matrices\n",
    "\n",
    "For a symmetric matrix, e'vals and e'vecs exist and are complete\n",
    "\n",
    "For general square matrix, not the case\n",
    "\n",
    "For rectangular matrix, certainly not\n",
    "\n",
    "Under singular value decomp: $A = U \\Sigma V^T$, where $\\Sigma = \\textrm{diag}(s_1, \\ldots, s_n)$ where $s_i$ are the 'singular values', and are all positive.\n",
    "\n",
    "### The details\n",
    "\n",
    "1. The key observation is that $A^T A$ is symmetric, square, and positive semi-definite.\n",
    "2. Thus: $A^T A$ can be decomposed: $A^T A = V \\Lambda V^T$, with $V$ orthogonal, and $\\Lambda$ positive.\n",
    "3. We also have $A A^T$ is symmetric and positive semi-definite, and we have $A A^T = U \\Lambda U^T$\n",
    "4. Now look for $A v_i = \\sigma_i u_i$, where the $v_i$ and $u_i$ are sets of orthogonal vectors.\n",
    "\n",
    "\"We're looking for one set of orthogonal vectors in the 'input space' of $A$, and a set of orthogonal vectors in the 'output space' of $A$ that transform to each other via $A$\".\n",
    "\n",
    "We then have $A V = U \\Sigma$, which then gives us $A = U \\Sigma V^T$.\n",
    "\n",
    "Now... what are the $V$s and what are the $U$s.\n",
    "\n",
    "We can see that if $A = U \\Sigma V^T$ then $A^T A = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^2 V^T = V \\Lambda V^T$ --- that is, the $V$s are the eigenvectors of $A^T A$, and the $\\Lambda = \\Sigma^2$ are the eigenvalues of $A^T A$. Similarly for $A A^T$.\n",
    "\n",
    "Haven't quite finished: need to deal with the case of repeated eigenvalues --- and hence have 'eigenspaces'. Need to pick the appropriate eigenvectors from these spaces to satisfy $A v_i = \\sigma_i u_i$.\n",
    "\n",
    "We do this fixing the $v$s to be a particular set of eigenvectors of $A^T A$, and then solving $u_i = A v_i / \\sigma_i$ - this ensures that whatever choices we make for the $v_i$, we get the appropriate $u_i$ in the degenerate cases.\n",
    "\n",
    "Finally just need to show that the $u_i$'s picked in this way are orthogonal: $u_i^T u_i = \\frac{v_i^T A^T A v_j}{\\sigma_i \\sigma_j} = v_i^T v_j \\frac{\\sigma_j^2}{\\sigma_i \\sigma_j} = \\delta_{i,j}$\n",
    "\n",
    "### Geometry of the SVD\n",
    "\n",
    "\"Every matrix factors into a rotation, then 'stretch', then rotation\"\n",
    "\n",
    "![test](./images/svd_geometry.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
